# Multi-agent bandit execution configuration
# 多个 agent 在同一样本上协同工作，使用多臂老虎机（MAB）算法动态选择最优 agent

multi_agent_bandit:
  description: >
    Run multiple agents collaboratively on the same sample, then use a
    multi-armed bandit (MAB) algorithm to dynamically select the best-performing
    agent based on historical rewards. The algorithm learns which agent performs
    best over time and gradually shifts selection probability towards better agents.

  # ===== Agent pool =====
  agents:
    - name: Qwen/Qwen3-14B
    - name: deepseek-ai/DeepSeek-V2.5
    - name: THUDM/GLM-4-32B-0414

  memory: independent # independent | shared
  
  # ===== Bandit algorithm =====
  bandit:
    method: decaying epsilon-greedy    # decaying_epsilon_greedy | upper_confidence_bound | thompson_sampling
    probability: 0.05                   # Only used for decaying_epsilon_greedy: initial exploration probability
    coefficient: 1                      # Only used for upper_confidence_bound: UCB exploration coefficient
    mab_memory_independent: False       # True: each agent has independent memory; False: shared memory across agents